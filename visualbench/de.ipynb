{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085cc497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counters reset.\n",
      "\n",
      "--- Test 1: Backward Pass ---\n",
      "Initial loss: 0.10916007310152054, Monitored loss: 0.10916007310152054\n",
      "Current Counts: Backward Pass = 0, HVP = 0, JVP = 0\n",
      "Current Counts: Backward Pass = 1, HVP = 0, JVP = 0\n",
      "Current Counts: Backward Pass = 2, HVP = 0, JVP = 0\n",
      "\n",
      "--- Test 2: HVP ---\n",
      "Counters reset.\n",
      "Current Counts: Backward Pass = 0, HVP = 1, JVP = 0\n",
      "Current Counts: Backward Pass = 0, HVP = 2, JVP = 0\n",
      "\n",
      "--- Test 3: JVP ---\n",
      "Counters reset.\n",
      "Current Counts: Backward Pass = 0, HVP = 0, JVP = 1\n",
      "Current Counts: Backward Pass = 0, HVP = 0, JVP = 2\n",
      "\n",
      "--- Test 4: Monitored scalar in HVP/JVP func ---\n",
      "Counters reset.\n",
      "Before HVP call:\n",
      "Current Counts: Backward Pass = 0, HVP = 0, JVP = 0\n",
      "After HVP call:\n",
      "Current Counts: Backward Pass = 2, HVP = 1, JVP = 0\n",
      "\n",
      "--- Test 5: VJP via counted_vjp ---\n",
      "Counters reset.\n",
      "Current Counts: Backward Pass = 1, HVP = 0, JVP = 0\n",
      "Counters reset.\n",
      "Current Counts: Backward Pass = 1, HVP = 0, JVP = 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd.functional import hvp as torch_hvp\n",
    "from torch.autograd.functional import jvp as torch_jvp\n",
    "from torch.autograd import grad as torch_grad\n",
    "\n",
    "class ComputationMonitor:\n",
    "    def __init__(self):\n",
    "        self.reset_counts()\n",
    "\n",
    "        # Define the custom autograd Function within the monitor\n",
    "        # so it can access the monitor's instance variables.\n",
    "        class _CounterAutograd(torch.autograd.Function):\n",
    "            @staticmethod\n",
    "            def forward(ctx, monitor_instance, scalar_tensor):\n",
    "                # Store the monitor instance for the backward pass\n",
    "                ctx.monitor_instance = monitor_instance\n",
    "                # We don't need to save scalar_tensor for backward if we're just passing grad_output\n",
    "                # Ensure the output has the same requires_grad status as the input\n",
    "                # If scalar_tensor doesn't require grad, this output won't either,\n",
    "                # and backward won't be called. This is usually desired.\n",
    "                return scalar_tensor.clone() # Use clone to ensure a new node in graph\n",
    "\n",
    "            @staticmethod\n",
    "            def backward(ctx, grad_output):\n",
    "                # Access the monitor instance stored in ctx\n",
    "                monitor = ctx.monitor_instance\n",
    "                monitor._increment_backward_passes()\n",
    "                # Pass the gradient through\n",
    "                return None, grad_output # Gradient for monitor_instance, gradient for scalar_tensor\n",
    "\n",
    "        self._CounterAutogradFn = _CounterAutograd\n",
    "\n",
    "    def _increment_backward_passes(self):\n",
    "        self.backward_passes_count += 1\n",
    "\n",
    "    def _increment_hvp_calls(self):\n",
    "        self.hvp_calls_count += 1\n",
    "\n",
    "    def _increment_jvp_calls(self):\n",
    "        self.jvp_calls_count += 1\n",
    "\n",
    "    def reset_counts(self):\n",
    "        self.backward_passes_count = 0\n",
    "        self.hvp_calls_count = 0\n",
    "        self.jvp_calls_count = 0\n",
    "        print(\"Counters reset.\")\n",
    "\n",
    "    def get_counts(self):\n",
    "        return {\n",
    "            \"backward_passes\": self.backward_passes_count,\n",
    "            \"hvp_calls\": self.hvp_calls_count,\n",
    "            \"jvp_calls\": self.jvp_calls_count,\n",
    "        }\n",
    "\n",
    "    def print_counts(self):\n",
    "        counts = self.get_counts()\n",
    "        print(f\"Current Counts: Backward Pass = {counts['backward_passes']}, \"\n",
    "              f\"HVP = {counts['hvp_calls']}, JVP = {counts['jvp_calls']}\")\n",
    "\n",
    "    def monitor_scalar(self, scalar_tensor: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Pass a scalar tensor through this function to count backward passes involving it.\n",
    "        The tensor MUST be a scalar and typically requires gradients (e.g., a loss).\n",
    "        \"\"\"\n",
    "        if not scalar_tensor.ndim == 0:\n",
    "            raise ValueError(\"Monitored tensor must be a scalar.\")\n",
    "        if not scalar_tensor.requires_grad:\n",
    "            # If it doesn't require grad, .backward() won't be called on it anyway.\n",
    "            # We could warn or let it pass. For now, let it pass, as our autograd\n",
    "            # function won't have its backward called either.\n",
    "            pass\n",
    "        return self._CounterAutogradFn.apply(self, scalar_tensor)\n",
    "\n",
    "    def counted_hvp(self, func, inputs, v, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Wrapper around torch.autograd.functional.hvp that increments HVP count.\n",
    "        \"\"\"\n",
    "        self._increment_hvp_calls()\n",
    "        return torch_hvp(func, inputs, v, *args, **kwargs)\n",
    "\n",
    "    def counted_jvp(self, func, inputs, v, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Wrapper around torch.autograd.functional.jvp that increments JVP count.\n",
    "        \"\"\"\n",
    "        self._increment_jvp_calls()\n",
    "        return torch_jvp(func, inputs, v, *args, **kwargs)\n",
    "\n",
    "    def counted_vjp(self, func_output_scalar, inputs, v=None, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Wrapper for VJP (which is essentially what .backward() or torch.autograd.grad does\n",
    "        for a scalar output). This can be used to count VJPs explicitly if you are\n",
    "        using torch.autograd.grad for what would be a backward pass.\n",
    "        Note: If you use monitor_scalar() and then loss.backward(), the backward_passes_count\n",
    "        will be incremented. This counted_vjp is for cases where you might compute\n",
    "        VJP using torch.autograd.grad directly on a scalar output.\n",
    "        \"\"\"\n",
    "        # This is a bit tricky. If func_output_scalar is the result of monitor_scalar,\n",
    "        # then calling .backward() or torch.autograd.grad will trigger the\n",
    "        # _CounterAutograd.backward.\n",
    "        # This method is more for if you're calling torch.autograd.grad on a function's\n",
    "        # scalar output *without* having passed it through monitor_scalar's autograd.Function.\n",
    "        # However, to be consistent, let's assume it's a separate way to count something \"like\" a backward.\n",
    "        # For now, let's count it as a backward pass, as VJP on a scalar output is dL/dx.\n",
    "        self._increment_backward_passes() # Or a separate counter if VJP is distinct for you\n",
    "        if v is None: # For scalar output, v defaults to torch.tensor(1.0)\n",
    "            v = torch.tensor(1.0, dtype=func_output_scalar.dtype, device=func_output_scalar.device)\n",
    "        return torch_grad(func_output_scalar, inputs, grad_outputs=v, *args, **kwargs)\n",
    "\n",
    "\n",
    "# --- Example Usage ---\n",
    "if __name__ == \"__main__\":\n",
    "    monitor = ComputationMonitor()\n",
    "\n",
    "    # --- Test 1: Backward Pass ---\n",
    "    print(\"\\n--- Test 1: Backward Pass ---\")\n",
    "    x = torch.randn(3, requires_grad=True)\n",
    "    y = torch.randn(3, requires_grad=True)\n",
    "    \n",
    "    # Some computation leading to a scalar loss\n",
    "    output = (x * y).sum() * 2\n",
    "    loss = output.sin() # loss is a scalar\n",
    "\n",
    "    # Monitor the loss\n",
    "    monitored_loss = monitor.monitor_scalar(loss)\n",
    "    \n",
    "    print(f\"Initial loss: {loss.item()}, Monitored loss: {monitored_loss.item()}\")\n",
    "    monitor.print_counts() # Expect 0 for all\n",
    "\n",
    "    # Perform backward pass\n",
    "    monitored_loss.backward()\n",
    "    monitor.print_counts() # Expect backward_passes = 1\n",
    "\n",
    "    # Perform another backward pass (e.g. with retain_graph=True if needed for loss)\n",
    "    # For simplicity, let's create a new graph path\n",
    "    x_new = torch.randn(3, requires_grad=True)\n",
    "    loss_new = (x_new**2).sum()\n",
    "    monitored_loss_new = monitor.monitor_scalar(loss_new)\n",
    "    monitored_loss_new.backward()\n",
    "    monitor.print_counts() # Expect backward_passes = 2\n",
    "\n",
    "    # --- Test 2: HVP ---\n",
    "    print(\"\\n--- Test 2: HVP ---\")\n",
    "    monitor.reset_counts()\n",
    "\n",
    "    def scalar_func_hvp(inp_x, inp_y):\n",
    "        return ((inp_x * inp_y).sum() * 2).sin()\n",
    "\n",
    "    inputs_hvp = (x.clone().detach().requires_grad_(True), y.clone().detach().requires_grad_(True))\n",
    "    v_hvp = (torch.randn_like(x), torch.randn_like(y))\n",
    "\n",
    "    # Call HVP using the monitor's wrapper\n",
    "    hvp_result, grad_result = monitor.counted_hvp(scalar_func_hvp, inputs_hvp, v_hvp)\n",
    "    monitor.print_counts() # Expect hvp_calls = 1\n",
    "\n",
    "    monitor.counted_hvp(scalar_func_hvp, inputs_hvp, v_hvp)\n",
    "    monitor.print_counts() # Expect hvp_calls = 2\n",
    "\n",
    "    # --- Test 3: JVP ---\n",
    "    print(\"\\n--- Test 3: JVP ---\")\n",
    "    monitor.reset_counts()\n",
    "\n",
    "    def func_jvp(inp_x): # JVP often used with functions returning non-scalars\n",
    "        return inp_x * 2 + inp_x.sin()\n",
    "\n",
    "    inputs_jvp = x.clone().detach().requires_grad_(True)\n",
    "    v_jvp = torch.randn_like(x)\n",
    "\n",
    "    # Call JVP using the monitor's wrapper\n",
    "    jvp_result = monitor.counted_jvp(func_jvp, (inputs_jvp,), (v_jvp,))\n",
    "    monitor.print_counts() # Expect jvp_calls = 1\n",
    "\n",
    "    monitor.counted_jvp(func_jvp, (inputs_jvp,), (v_jvp,))\n",
    "    monitor.print_counts() # Expect jvp_calls = 2\n",
    "\n",
    "    # --- Test 4: Monitored scalar involved in HVP/JVP function (indirect) ---\n",
    "    print(\"\\n--- Test 4: Monitored scalar in HVP/JVP func ---\")\n",
    "    monitor.reset_counts()\n",
    "\n",
    "    p = torch.tensor([2.0, 3.0], requires_grad=True)\n",
    "    q = torch.tensor([0.5, 1.5], requires_grad=True)\n",
    "\n",
    "    def outer_func_for_hvp(param_p, param_q):\n",
    "        intermediate_scalar = (param_p * param_q).sum()\n",
    "        monitored_intermediate = monitor.monitor_scalar(intermediate_scalar)\n",
    "        # The HVP is on 'outer_func_for_hvp'.\n",
    "        # If 'monitored_intermediate' itself is differentiated (e.g., if outer_func_for_hvp\n",
    "        # was part of a larger graph and we called .backward() on its output),\n",
    "        # then backward_passes_count would increment.\n",
    "        # Here, HVP computes derivatives *through* monitored_intermediate.\n",
    "        # The .backward() of _CounterAutograd will be called during HVP's internal backward passes.\n",
    "        return monitored_intermediate**2\n",
    "\n",
    "    v_hvp2 = (torch.randn_like(p), torch.randn_like(q))\n",
    "    \n",
    "    print(\"Before HVP call:\")\n",
    "    monitor.print_counts()\n",
    "\n",
    "    # HVP involves second derivatives. The backward pass of _CounterAutogradFn\n",
    "    # will be called when computing the gradient (first part of HVP),\n",
    "    # and potentially again depending on how HVP is implemented (e.g. if it does grad of grad).\n",
    "    # torch.autograd.functional.hvp typically uses one backward pass to get VJP (g),\n",
    "    # then one JVP-like operation (grad(g @ v)) which might involve another backward pass.\n",
    "    hvp_res, _ = monitor.counted_hvp(outer_func_for_hvp, (p, q), v_hvp2)\n",
    "    \n",
    "    print(\"After HVP call:\")\n",
    "    monitor.print_counts() # Expect hvp_calls = 1, backward_passes might be >0 (typically 2 for HVP)\n",
    "\n",
    "    # --- Test 5: VJP via counted_vjp ---\n",
    "    print(\"\\n--- Test 5: VJP via counted_vjp ---\")\n",
    "    monitor.reset_counts()\n",
    "    a = torch.tensor(2.0, requires_grad=True)\n",
    "    b = torch.tensor(3.0, requires_grad=True)\n",
    "    \n",
    "    def my_scalar_output_func(in_a, in_b):\n",
    "        return (in_a * in_b).sin()\n",
    "\n",
    "    # Case 1: Func output is NOT monitored by monitor_scalar\n",
    "    scalar_out = my_scalar_output_func(a, b)\n",
    "    # We want to count this torch.autograd.grad call as a \"backward pass\" like event\n",
    "    grads_ab = monitor.counted_vjp(scalar_out, (a,b))\n",
    "    monitor.print_counts() # Expect backward_passes = 1 (due to counted_vjp)\n",
    "\n",
    "    # Case 2: Func output IS monitored by monitor_scalar\n",
    "    # This will double count if not careful: once by _CounterAutogradFn.backward,\n",
    "    # once by counted_vjp.\n",
    "    # The primary mechanism for .backward() style counting is monitor_scalar.\n",
    "    # counted_vjp is for explicit torch.autograd.grad calls on non-monitored scalars.\n",
    "    monitor.reset_counts()\n",
    "    scalar_out_monitored = monitor.monitor_scalar(my_scalar_output_func(a,b))\n",
    "    # Now, if we use torch.autograd.grad:\n",
    "    # This will trigger _CounterAutogradFn.backward()\n",
    "    grads_ab_monitored = torch_grad(scalar_out_monitored, (a,b))\n",
    "    monitor.print_counts() # Expect backward_passes = 1 (due to monitor_scalar's effect)\n",
    "                           # and JVP/HVP = 0.\n",
    "\n",
    "    # If you were to use monitor.counted_vjp on scalar_out_monitored, it would add another\n",
    "    # to backward_passes_count. So, use one or the other for counting a VJP.\n",
    "    # monitor.counted_vjp(scalar_out_monitored, (a,b)) # This would make backward_passes = 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
