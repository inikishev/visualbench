{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pylint:disable=not-callable\n",
    "from typing import Literal\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from myai.transforms import tonumpy, totensor\n",
    "\n",
    "from visualbench.benchmark import Benchmark\n",
    "\n",
    "\n",
    "def put_alpha(x: np.ndarray, other:np.ndarray, alpha1: float, alpha2: float = 1):\n",
    "    return x - (x - other)*(alpha1*alpha2)\n",
    "\n",
    "def softrect2d_(array: np.ndarray, x1, x2, color, alpha: float, add_fn = put_alpha) -> None:\n",
    "    \"\"\"same as array[x1[0]:x2[0], x1[1]:x2[1]] = color, but with a soft edge\n",
    "\n",
    "    Args:\n",
    "        array (np.ndarray): a (H, W, 3) array\n",
    "        x1 (_type_): coord of first point.\n",
    "        x2 (_type_): coord of second point.\n",
    "        color (_type_): color - 3 floats\n",
    "        alpha (float): alpha\n",
    "        add_fn (_type_, optional): function that adds. Defaults to put_alpha.\n",
    "    \"\"\"\n",
    "    x1 = np.clip(tonumpy(x1), 0, array.shape[:-1])\n",
    "    x1low = np.floor(x1,).astype(int)\n",
    "    x1high = np.ceil(x1,).astype(int)\n",
    "    x1dist_from_low = x1 - x1low\n",
    "\n",
    "    x2 = np.clip(tonumpy(x2), 0, array.shape[:-1])\n",
    "    x2low = np.floor(x2,).astype(int)\n",
    "    x2high = np.ceil(x2,).astype(int)\n",
    "    x2dist_from_low = x2 - x2low\n",
    "\n",
    "    color = tonumpy(color)\n",
    "\n",
    "    if x1dist_from_low[0] > 0:\n",
    "        array[x1low[0], x1high[1]:x2low[1]] = add_fn(array[x1low[0], x1high[1]:x2low[1]], color, 1-x1dist_from_low[0], alpha)\n",
    "    if x1dist_from_low[1] > 0:\n",
    "        array[x1high[0]:x2low[0], x1low[1]] = add_fn(array[x1high[0]:x2low[0], x1low[1]], color, 1-x1dist_from_low[1], alpha)\n",
    "    if x2dist_from_low[0] > 0:\n",
    "        array[x2high[0]-1, x1high[1]:x2low[1]] = add_fn(array[x2high[0]-1, x1high[1]:x2low[1]], color, x2dist_from_low[0], alpha)\n",
    "    if x2dist_from_low[1] > 0:\n",
    "        array[x1high[0]:x2low[0], x2high[1]-1] = add_fn(array[x1high[0]:x2low[0], x2high[1]-1], color, x2dist_from_low[1], alpha)\n",
    "\n",
    "    # fill main rectangle\n",
    "    array[x1high[0]:x2low[0], x1high[1]:x2low[1]] = add_fn(array[x1high[0]:x2low[0], x1high[1]:x2low[1]], color, alpha, 1)\n",
    "\n",
    "CONTAINER1 = (10,10), ((1,10), (1,10), (1,9), (1,9), (3,3), (2,4), (4,1), (3,4), (3,2), (2,2), (1,1), (1,1), (6,1), (8,1))\n",
    "\n",
    "def uniform_container(box_size:tuple[float,float], num_boxes:tuple[int,int]):\n",
    "    \"\"\"makes a container filled with same sized boxes.\n",
    "\n",
    "    Args:\n",
    "        box_size (tuple[float,float]): _description_\n",
    "        num_boxes (tuple[int,int]): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    container_size = (box_size[0] * num_boxes[0], box_size[1] * num_boxes[1])\n",
    "    boxes = [box_size for _ in range(num_boxes[0] * num_boxes[1])]\n",
    "    return container_size, boxes\n",
    "\n",
    "class BoxPacking(Benchmark):\n",
    "    def __init__(\n",
    "        self,\n",
    "        container_size = CONTAINER1[0],\n",
    "        box_sizes = CONTAINER1[1],\n",
    "        npixels: float | None = 100_000,\n",
    "        square: bool = False,\n",
    "        penalty: float = 0.5,\n",
    "        sq_penalty: float = 20,\n",
    "        init: Literal['center', 'corner', 'random', 'top'] = 'top',\n",
    "        colors_seed: int | None = 2,\n",
    "        dtype = torch.float32,\n",
    "        device: torch.types.Device = 'cpu', # faster on cpu\n",
    "    ):\n",
    "        \"\"\"Box packing benchmark, can be rendered as a video. If possible, params should be bounded to 0-1 range.\n",
    "\n",
    "        Args:\n",
    "            container_size (_type_, optional): tuple of two numbers - size of the container to fit boxes into. Defaults to CONTAINER1[0].\n",
    "            box_sizes (_type_, optional): list of tuples of two numbers per box - its x and y size. Defaults to CONTAINER1[1].\n",
    "            npixels (float | None, optional):\n",
    "                Number of pixels in the video (product of width and height). Defaults to 100_000.\n",
    "                Aspect ratio is determined by `container_size`.\n",
    "            square (bool, optional): if True, overlap in loss function will be squared. Defaults to False.\n",
    "            penalty (float, optional):\n",
    "                multiplier to absolute penalty for going outside the edges. Defaults to 0.5.\n",
    "            sq_penalty (float, optional):\n",
    "                multiplier to squared penalty for going outside the edges. Defaults to 20.\n",
    "            init (str, optional):\n",
    "                initially put boxes in the center, in the corner, or randomly.\n",
    "                'random' init is seeded and is always the same. Other inits\n",
    "                also add a very small amount of seeded noise to ensure no two boxes\n",
    "                spawn in exactly the same place, which would make their gradients\n",
    "                identical so they will never separate. Defaults to 'random'.\n",
    "            colors_seed (int, optional): seed for box colors. Defaults to 2.\n",
    "            dtype (dtype, optional): dtype. Defaults to torch.float32.\n",
    "            device (Device, optional): device. Defaults to 'cpu'.\n",
    "        \"\"\"\n",
    "        if npixels is not None: scale = (npixels / np.prod(container_size)) ** (1/2)\n",
    "        else: scale = 1\n",
    "        self.scale = scale\n",
    "        self.container_size_np = (np.array(container_size, dtype = float) * scale).astype(int)\n",
    "        self.size = torch.prod(torch.tensor(self.container_size_np, dtype = dtype, device = device))\n",
    "        self.container_size = torch.from_numpy(self.container_size_np).to(dtype=dtype, device=device)\n",
    "        self.box_sizes = totensor(box_sizes, dtype = dtype, device = device) * scale\n",
    "        self.box_sizes_np = self.box_sizes.detach().cpu().numpy()\n",
    "        self.square = square\n",
    "\n",
    "        self.penalty = penalty\n",
    "        self.sq_penalty = sq_penalty\n",
    "\n",
    "        # generate colors for boxes\n",
    "        colors = []\n",
    "        n = 2\n",
    "        while len(colors) < len(box_sizes):\n",
    "            colors = list(itertools.product(np.linspace(0, 255, n), repeat=3))\n",
    "            if (255., 255., 255.) in colors: colors.remove((255., 255., 255.))\n",
    "            n+=1\n",
    "\n",
    "        rng = random.Random(colors_seed)\n",
    "        self.colors = rng.sample(colors, k = len(box_sizes))\n",
    "\n",
    "        # remove almost white colors\n",
    "        for i in range(len(self.colors)): # pylint:disable=consider-using-enumerate\n",
    "            while sum(self.colors[i]) > 600:\n",
    "                self.colors[i] = rng.sample(colors, k = 1)[0]\n",
    "        super().__init__(log_params=True, save_edge_params=True, device=device)\n",
    "\n",
    "        # slightly randomize params so that no params overlap which gives them exactly the same gradients\n",
    "        # so they never detach from each other\n",
    "        normalized_box_sizes = self.box_sizes / self.container_size.unsqueeze(0) # 0 to 1\n",
    "        generator = torch.Generator(device).manual_seed(0)\n",
    "        noise = torch.randn((len(box_sizes), 2), dtype = dtype, device = device, generator=generator)\n",
    "\n",
    "        if init == 'center':\n",
    "            self.params = torch.nn.Parameter((1 - normalized_box_sizes) * 0.5 + noise.mul(0.01), requires_grad=True)\n",
    "        elif init == 'corner':\n",
    "            self.params = torch.nn.Parameter(noise.uniform_(0, 0.01), requires_grad=True)\n",
    "        elif init == 'top':\n",
    "            p = (1 - normalized_box_sizes) * 0.5 + noise.mul(0.01)\n",
    "            p[:, 0] = noise.uniform_(0, 0.01)[:,0]\n",
    "            self.params = torch.nn.Parameter(p, requires_grad=True)\n",
    "        elif init == 'random':\n",
    "            self.params = torch.nn.Parameter((1 - normalized_box_sizes) * noise.uniform_(0, 1))\n",
    "\n",
    "    def _make_solution_image(self, paramvec: torch.Tensor):\n",
    "        arr = paramvec.detach().cpu().numpy().reshape(self.params.shape)\n",
    "        container = np.full((*self.container_size_np, 3), 255)\n",
    "        for (y,x), box, c in zip(arr, self.box_sizes_np, self.colors):\n",
    "            y *= self.container_size_np[0]\n",
    "            y *= (self.container_size_np[0] - box[0])/self.container_size_np[0]\n",
    "            x *= self.container_size_np[1]\n",
    "            x *= (self.container_size_np[1] - box[1])/self.container_size_np[1]\n",
    "            # if y+box[0] >= self.container_size_np[0]: y = self.container_size_np[0] - box[0]\n",
    "            # if x+box[1] >= self.container_size_np[1]: x = self.container_size_np[1] - box[1]\n",
    "            try: softrect2d_(container, (y,x), (y+box[0], x+box[1]), c, 0.5,)\n",
    "            except IndexError: pass\n",
    "        return np.clip(container, 0, 255).astype(np.uint8)\n",
    "\n",
    "    def forward(self):\n",
    "        # we still need penalty as if box is entirely outside, gradient will be 0\n",
    "        #overflows = [self.params[self.params>1] - 1, self.params[self.params < 0]]\n",
    "        overflows = [torch.where(self.params > 1, self.params - 1, 0), torch.where(self.params < 0, self.params, 0)]\n",
    "        penalty = torch.stack([i.abs().mean() for i in overflows]).sum() * self.penalty\n",
    "        penalty = penalty + torch.stack([i.pow(2).mean() for i in overflows]).sum() * self.sq_penalty\n",
    "        #if not torch.isfinite(penalty): penalty = torch.tensor(0, device = self.params.device)\n",
    "\n",
    "        # create boxes from parameters\n",
    "        params = self.params\n",
    "        boxes = torch.zeros(len(self.box_sizes)+4, 4, device = self.params.device)\n",
    "        for i, ((y,x), box) in enumerate(zip(params, self.box_sizes)):\n",
    "            y = y * self.container_size[0]\n",
    "            y = y * (self.container_size[0] - box[0])/self.container_size[0]\n",
    "            x = x * self.container_size[1]\n",
    "            x = x * (self.container_size[1] - box[1])/self.container_size[1]\n",
    "\n",
    "            boxes[i, 0] = y; boxes[i, 1] = y+box[0]; boxes[i, 2] = x; boxes[i, 3] = x+box[1]\n",
    "\n",
    "        # edge boxes\n",
    "        for i, edge in enumerate([\n",
    "            (-1e10, 0, -1e10, 0),\n",
    "            (-1e10, 0, self.container_size[1], 1e10),\n",
    "            (self.container_size[0], 1e10, -1e10, 0),\n",
    "            (self.container_size[0], 1e10, self.container_size[1], 1e10),\n",
    "        ]):\n",
    "            ip = i+1\n",
    "            boxes[-ip, 0] = edge[0]; boxes[-ip, 1] = edge[1]; boxes[-ip, 2] = edge[2]; boxes[-ip, 3] = edge[3]\n",
    "\n",
    "        # this calculates total overlap between every pair of boxes\n",
    "        # but in a vectorized way\n",
    "        ya1, yb1 = torch.meshgrid(boxes[:, 0], boxes[:, 0], indexing = 'ij')\n",
    "        ya2, yb2 = torch.meshgrid(boxes[:, 1], boxes[:, 1], indexing = 'ij')\n",
    "        xa1, xb1 = torch.meshgrid(boxes[:, 2], boxes[:, 2], indexing = 'ij')\n",
    "        xa2, xb2 = torch.meshgrid(boxes[:, 3], boxes[:, 3], indexing = 'ij')\n",
    "\n",
    "        x_overlap = torch.clamp(torch.minimum(xa2, xb2) - torch.maximum(xa1, xb1), min=0)\n",
    "\n",
    "        # mask diagonal elements (ovelap with itself), and last four boxes as those are to avoid overflow overedges\n",
    "        mask = torch.eye(len(boxes), dtype = torch.bool, device = self.params.device).logical_not_()\n",
    "        mask[-4:] = False\n",
    "        y_overlap = torch.clamp(torch.minimum(ya2, yb2) - torch.maximum(ya1, yb1), min=0) * mask\n",
    "\n",
    "        overlap = x_overlap * y_overlap\n",
    "        if self.square: overlap = overlap ** 2\n",
    "\n",
    "        loss = overlap.sum() / self.size\n",
    "        penalized_loss = loss + penalty\n",
    "\n",
    "        # code above is equivalent to commented out code below (which was very slow):\n",
    "        # for i, (ya1, ya2, xa1, xa2) in enumerate(boxes):\n",
    "        #     for j, (yb1, yb2, xb1, xb2) in enumerate(boxes[:-4]): # skip last 4 edge boxes\n",
    "        #         if i != j:\n",
    "        #             x_overlap = max(min(xa2, xb2) - max(xa1, xb1), 0)\n",
    "        #             y_overlap = max(min(ya2, yb2) - max(ya1, yb1), 0)\n",
    "        #             overlap = x_overlap * y_overlap\n",
    "        #             if self.square: overlap = overlap ** 2\n",
    "        #             loss = loss + overlap / self.size\n",
    "\n",
    "        return penalized_loss, {\"overlap\": loss, \"penalty\": penalty}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Callable\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from myai.transforms import normalize\n",
    "\n",
    "\n",
    "@torch.no_grad\n",
    "def normalize_to_uint8(x:torch.Tensor | np.ndarray):\n",
    "    if isinstance(x, np.ndarray): return normalize(x, 0, 255).astype(np.uint8)\n",
    "    return normalize(x.detach(), 0, 255).cpu().numpy().astype(np.uint8)\n",
    "\n",
    "def l1(x,y):\n",
    "    return (x-y).abs().mean()\n",
    "\n",
    "\n",
    "class MatrixInverse(Benchmark):\n",
    "    def __init__(self, mat: torch.Tensor, loss: Callable = l1, dtype: torch.dtype=torch.float32, device: torch.types.Device='cuda'):\n",
    "        \"\"\"Finding inverse of a matrix. This supports video rendering.\n",
    "\n",
    "        Args:\n",
    "            mat (torch.Tensor):\n",
    "                square matrix, can have additional first channels dimension which is treated as batch dimension.\n",
    "            loss (Callable, optional): final loss is `loss(A@B, B@A) + loss(A@B, I) + loss(B@A, I) + loss(diag(B@A), 1) + loss(diag(A@B), 1)`. Defaults to l1.\n",
    "            dtype (dtype, optional): dtype. Defaults to torch.float32.\n",
    "            device (Device, optional): device. Defaults to 'cuda'.\n",
    "        \"\"\"\n",
    "        if mat.shape[-1] != mat.shape[-2]: raise ValueError(f'{mat.shape = } - not a matrix!')\n",
    "        self.mat = mat.to(dtype = dtype, device = device, memory_format = torch.contiguous_format)\n",
    "        self.loss_fn = loss\n",
    "\n",
    "        mat_reference = normalize_to_uint8(mat)\n",
    "        labels = ['input']\n",
    "\n",
    "        try:\n",
    "            true_inv = torch.linalg.inv(self.mat).cpu().numpy().astype(np.uint8) # pylint:disable=not-callable\n",
    "            labels.append('true inverse')\n",
    "        except torch.linalg.LinAlgError as e:\n",
    "            true_inv = torch.linalg.pinv(self.mat) # pylint:disable=not-callable\n",
    "            labels.append('pseudoinverse')\n",
    "\n",
    "        true_inv_reference = normalize_to_uint8(true_inv)\n",
    "\n",
    "        super().__init__(reference_images = [mat_reference, true_inv_reference], reference_labels = labels, save_edge_params = True, device=device, seed=0)\n",
    "        self.inverse = torch.nn.Parameter(self.mat.clone().requires_grad_(True))\n",
    "\n",
    "    def forward(self):\n",
    "        AB = self.mat @ self.inverse\n",
    "        BA = self.inverse @ self.mat\n",
    "        I = torch.eye(self.mat.shape[-1], device = AB.device, dtype=AB.dtype)\n",
    "        I_diag = torch.ones(BA.shape[-1], device = AB.device, dtype=AB.dtype)\n",
    "        loss = self.loss_fn(AB, BA)  +\\\n",
    "            self.loss_fn(AB, I) +\\\n",
    "            self.loss_fn(BA, I) +\\\n",
    "            self.loss_fn(BA.diagonal(0,-2,-1), I_diag) +\\\n",
    "            self.loss_fn(AB.diagonal(0,-2,-1), I_diag)\n",
    "\n",
    "        return loss, {\"image_output\": normalize_to_uint8(self.inverse), \"image_AB\": normalize_to_uint8(AB), \"image_BA\": normalize_to_uint8(BA),}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BatchedTensor(lvl=1, bdim=0, value=\n",
      "    tensor([[[-1.5796, -0.2776, -0.2834,  ...,  1.0983, -0.1605, -1.3178],\n",
      "             [-1.0092, -0.3907,  0.1353,  ...,  0.3787,  0.0734,  0.0075],\n",
      "             [-1.1923, -0.8735,  2.0364,  ...,  0.2830, -0.0993, -0.5837],\n",
      "             ...,\n",
      "             [-2.0453,  0.4329, -0.7350,  ...,  0.5257, -0.9012,  0.0823],\n",
      "             [-1.2831,  1.6514, -0.3431,  ...,  1.3604,  0.8584, -0.5121],\n",
      "             [ 0.3765,  0.0267,  0.5711,  ..., -0.0956, -0.6163,  2.0701]],\n",
      "\n",
      "            [[-0.6738, -0.5592,  0.3954,  ...,  1.4435,  0.4883, -1.0429],\n",
      "             [ 1.6575, -1.0167,  0.2256,  ...,  0.3530, -0.2716,  0.3330],\n",
      "             [ 0.4317, -0.1647, -0.7391,  ..., -1.0254,  0.1815, -0.5324],\n",
      "             ...,\n",
      "             [-0.3716, -0.5071,  1.0021,  ...,  0.0770, -0.0718,  0.6969],\n",
      "             [-0.6059,  0.3937, -0.3002,  ...,  2.1924, -0.4380,  0.3306],\n",
      "             [ 0.0619, -0.5377,  0.6062,  ..., -0.3087,  0.2335, -0.2223]],\n",
      "\n",
      "            [[ 0.0273,  0.4670, -0.8097,  ...,  0.0344,  1.7108, -1.6485],\n",
      "             [-0.9863, -1.1227, -0.2690,  ...,  0.1236,  0.2926,  0.8087],\n",
      "             [ 0.1365,  0.0787,  0.9382,  ..., -1.0214, -1.0086, -0.3739],\n",
      "             ...,\n",
      "             [-0.0546,  1.2645, -0.5075,  ..., -0.0649,  0.1971, -1.4813],\n",
      "             [-1.5159,  0.5562, -1.2130,  ...,  0.3356, -1.5191,  0.4532],\n",
      "             [ 0.5786,  0.6838,  1.9296,  ..., -0.4770,  1.3791,  0.2687]],\n",
      "\n",
      "            ...,\n",
      "\n",
      "            [[ 1.0858,  0.7983, -0.6826,  ..., -0.8497, -0.5388, -2.0800],\n",
      "             [-0.6288,  0.1067,  0.3536,  ...,  0.4825, -0.5909,  1.7426],\n",
      "             [-1.8861, -0.2001, -0.5545,  ...,  2.8072,  1.3604,  1.2429],\n",
      "             ...,\n",
      "             [ 0.4036, -1.5785, -0.3331,  ..., -1.4204, -0.0869, -0.6285],\n",
      "             [-0.0840, -1.6138, -0.9791,  ..., -1.2853,  0.2228,  0.8037],\n",
      "             [ 1.2136,  0.8704,  0.9663,  ..., -0.0148, -1.3138, -0.2376]],\n",
      "\n",
      "            [[-0.4455, -0.3933, -1.5259,  ..., -0.1795,  0.3133,  0.8946],\n",
      "             [ 0.5594, -1.5555, -1.1808,  ..., -0.7156, -0.3819,  2.1875],\n",
      "             [ 1.1384,  0.3317, -0.3128,  ..., -1.2442, -1.4716,  0.0324],\n",
      "             ...,\n",
      "             [ 0.4924,  1.1740,  0.1423,  ...,  0.3837, -0.3334, -1.1255],\n",
      "             [ 0.5952, -0.7413, -1.0223,  ..., -0.4730,  0.6880, -0.9419],\n",
      "             [-0.1200,  0.1500,  1.2589,  ...,  1.0886,  0.6595, -0.5098]],\n",
      "\n",
      "            [[ 1.9670,  0.1035,  1.2038,  ..., -0.5286,  0.0064,  0.5747],\n",
      "             [-1.1519,  1.2446,  0.4843,  ...,  0.8569, -0.4227,  0.4996],\n",
      "             [ 0.6471, -1.8898, -0.7362,  ...,  0.4704, -1.9116,  1.7949],\n",
      "             ...,\n",
      "             [ 0.4045, -1.1211,  0.6252,  ...,  0.6894,  0.0654, -0.7480],\n",
      "             [-1.2521,  0.8307, -0.3701,  ..., -0.5928,  0.4078,  2.3604],\n",
      "             [-0.1921,  0.1116, -1.3940,  ..., -1.7107, -0.5333, -0.3651]]],\n",
      "           device='cuda:0')\n",
      ")]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "vmap: It looks like you're attempting to use a Tensor in some data-dependent control flow. We don't support that yet, please shout over at https://github.com/pytorch/functorch/issues/257 .",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 27\u001b[0m\n\u001b[0;32m     25\u001b[0m vmapped_param_closure \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mvmap(param_closure)\n\u001b[0;32m     26\u001b[0m stacked_params \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mrandn((\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m*\u001b[39mi\u001b[38;5;241m.\u001b[39msize()), device\u001b[38;5;241m=\u001b[39mi\u001b[38;5;241m.\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mi\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m params]\n\u001b[1;32m---> 27\u001b[0m vmapped_param_closure(\u001b[38;5;241m*\u001b[39mstacked_params)\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\pytorch312\\Lib\\site-packages\\torch\\_functorch\\apis.py:203\u001b[0m, in \u001b[0;36mvmap.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 203\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m vmap_impl(\n\u001b[0;32m    204\u001b[0m         func, in_dims, out_dims, randomness, chunk_size, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    205\u001b[0m     )\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\pytorch312\\Lib\\site-packages\\torch\\_functorch\\vmap.py:331\u001b[0m, in \u001b[0;36mvmap_impl\u001b[1;34m(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\u001b[0m\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _chunked_vmap(\n\u001b[0;32m    321\u001b[0m         func,\n\u001b[0;32m    322\u001b[0m         flat_in_dims,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    328\u001b[0m     )\n\u001b[0;32m    330\u001b[0m \u001b[38;5;66;03m# If chunk_size is not specified.\u001b[39;00m\n\u001b[1;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _flat_vmap(\n\u001b[0;32m    332\u001b[0m     func,\n\u001b[0;32m    333\u001b[0m     batch_size,\n\u001b[0;32m    334\u001b[0m     flat_in_dims,\n\u001b[0;32m    335\u001b[0m     flat_args,\n\u001b[0;32m    336\u001b[0m     args_spec,\n\u001b[0;32m    337\u001b[0m     out_dims,\n\u001b[0;32m    338\u001b[0m     randomness,\n\u001b[0;32m    339\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    340\u001b[0m )\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\pytorch312\\Lib\\site-packages\\torch\\_functorch\\vmap.py:479\u001b[0m, in \u001b[0;36m_flat_vmap\u001b[1;34m(func, batch_size, flat_in_dims, flat_args, args_spec, out_dims, randomness, **kwargs)\u001b[0m\n\u001b[0;32m    475\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m vmap_increment_nesting(batch_size, randomness) \u001b[38;5;28;01mas\u001b[39;00m vmap_level:\n\u001b[0;32m    476\u001b[0m     batched_inputs \u001b[38;5;241m=\u001b[39m _create_batched_inputs(\n\u001b[0;32m    477\u001b[0m         flat_in_dims, flat_args, vmap_level, args_spec\n\u001b[0;32m    478\u001b[0m     )\n\u001b[1;32m--> 479\u001b[0m     batched_outputs \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39mbatched_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    480\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _unwrap_batched(batched_outputs, out_dims, vmap_level, batch_size, func)\n",
      "Cell \u001b[1;32mIn[6], line 16\u001b[0m, in \u001b[0;36mparam_closure\u001b[1;34m(*new_params)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m old_p, new_p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(params, new_params):\n\u001b[0;32m     14\u001b[0m     torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mswap_tensors(old_p, new_p)\n\u001b[1;32m---> 16\u001b[0m value \u001b[38;5;241m=\u001b[39m model()\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# swap params back to original ones\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m old_p, new_p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(params, new_params):\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\pytorch312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\pytorch312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[5], line 59\u001b[0m, in \u001b[0;36mMatrixInverse.forward\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     52\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_fn(AB, BA)  \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_fn(AB, I) \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_fn(BA, I) \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_fn(BA\u001b[38;5;241m.\u001b[39mdiagonal(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), I_diag) \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_fn(AB\u001b[38;5;241m.\u001b[39mdiagonal(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), I_diag)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparameters()))\n\u001b[1;32m---> 59\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage_output\u001b[39m\u001b[38;5;124m\"\u001b[39m: normalize_to_uint8(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minverse), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage_AB\u001b[39m\u001b[38;5;124m\"\u001b[39m: normalize_to_uint8(AB), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage_BA\u001b[39m\u001b[38;5;124m\"\u001b[39m: normalize_to_uint8(BA),}\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\pytorch312\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[1;32mIn[5], line 11\u001b[0m, in \u001b[0;36mnormalize_to_uint8\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mno_grad\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnormalize_to_uint8\u001b[39m(x:torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m|\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, np\u001b[38;5;241m.\u001b[39mndarray): \u001b[38;5;28;01mreturn\u001b[39;00m normalize(x, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39muint8)\n\u001b[1;32m---> 11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m normalize(x\u001b[38;5;241m.\u001b[39mdetach(), \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39muint8)\n",
      "File \u001b[1;32mF:\\Stuff\\Programming\\projects\\myai\\project\\src\\myai\\transforms\\intensity.py:146\u001b[0m, in \u001b[0;36mnormalize\u001b[1;34m(x, min, max)\u001b[0m\n\u001b[0;32m    144\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m-\u001b[39m x\u001b[38;5;241m.\u001b[39mmin()\n\u001b[0;32m    145\u001b[0m xmax \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mmax()\n\u001b[1;32m--> 146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m xmax \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m: x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m/\u001b[39m xmax\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x \u001b[38;5;241m*\u001b[39m (\u001b[38;5;28mmax\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mmin\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mmin\u001b[39m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: vmap: It looks like you're attempting to use a Tensor in some data-dependent control flow. We don't support that yet, please shout over at https://github.com/pytorch/functorch/issues/257 ."
     ]
    }
   ],
   "source": [
    "import torch, torch.nn as nn\n",
    "model = MatrixInverse(torch.randn(32,32))\n",
    "\n",
    "\n",
    "params = tuple(model.parameters())\n",
    "inputs = torch.randn(4, 10)\n",
    "targets = torch.randn(4, 1)\n",
    "\n",
    "def param_closure(*new_params):\n",
    "    \"\"\"closure that takes in params as inputs to work with torch.func.jvp.\n",
    "    For example an optimizer can create this from normal closure\"\"\"\n",
    "    # swap params to new params\n",
    "    for old_p, new_p in zip(params, new_params):\n",
    "        torch.utils.swap_tensors(old_p, new_p)\n",
    "\n",
    "    value = model()\n",
    "\n",
    "    # swap params back to original ones\n",
    "    for old_p, new_p in zip(params, new_params):\n",
    "        torch.utils.swap_tensors(old_p, new_p)\n",
    "\n",
    "    return value\n",
    "\n",
    "\n",
    "vmapped_param_closure = torch.vmap(param_closure)\n",
    "stacked_params = [torch.randn((32, *i.size()), device=i.device, dtype=i.dtype) for i in params]\n",
    "vmapped_param_closure(*stacked_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
