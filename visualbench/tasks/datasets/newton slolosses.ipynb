{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Iterable, Sequence\n",
    "from typing import Literal\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "def jacobian(input: Sequence[torch.Tensor], wrt: Sequence[torch.Tensor], create_graph=False):\n",
    "    flat_input = torch.cat([i.reshape(-1) for i in input])\n",
    "    return torch.autograd.grad(\n",
    "        flat_input,\n",
    "        wrt,\n",
    "        torch.eye(len(flat_input), device=input[0].device, dtype=input[0].dtype),\n",
    "        retain_graph=True,\n",
    "        create_graph=create_graph,\n",
    "        allow_unused=True,\n",
    "        is_grads_batched=True,\n",
    "    )\n",
    "\n",
    "\n",
    "def make_newton_loss(loss_fn, tik_l: float | Literal['eig'] = 1e-2, use_torch_func = False):\n",
    "\n",
    "    class NewtonLoss(torch.autograd.Function):\n",
    "\n",
    "        @staticmethod\n",
    "        def forward(ctx, preds: torch.Tensor, targets: torch.Tensor):\n",
    "            with torch.enable_grad():\n",
    "                # necessary to flatten preds FIRST so they are part of the graph\n",
    "                preds_flat = preds.ravel()\n",
    "                value = loss_fn(preds_flat.view_as(preds), targets)\n",
    "\n",
    "                # caluclate gradient and hessian\n",
    "                if use_torch_func:\n",
    "                    H: torch.Tensor = torch.func.hessian(loss_fn, 0)(preds_flat, targets) # pyright:ignore[reportAssignmentType]\n",
    "                    g = torch.autograd.grad(value, preds)[0]\n",
    "\n",
    "                else:\n",
    "                    g = torch.autograd.grad(value, preds_flat, create_graph=True)[0]\n",
    "                    H: torch.Tensor = jacobian([g], [preds_flat])[0]\n",
    "\n",
    "            # apply regularization\n",
    "            if tik_l == 'eig':\n",
    "                reg = torch.linalg.eigvalsh(H).neg().clip(min=0).max()\n",
    "            else:\n",
    "                reg = tik_l\n",
    "\n",
    "            if reg != 0:\n",
    "                H.add_(torch.eye(H.size(0), device=H.device, dtype=H.dtype).mul_(reg))\n",
    "\n",
    "            # newton step\n",
    "            newton_step, success = torch.linalg.solve_ex(H, g)\n",
    "            ctx.save_for_backward(newton_step.view_as(preds))\n",
    "\n",
    "            return value\n",
    "\n",
    "        @staticmethod\n",
    "        def backward(ctx, *grad_outputs):\n",
    "            newton_step = ctx.saved_tensors[0] # inputs to loss\n",
    "            return newton_step, None\n",
    "\n",
    "    return NewtonLoss.apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0773bd62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.5630, device='cuda:0', grad_fn=<NewtonLossBackward>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-4.9787e-03, -7.1071e-01, -1.5677e+00, -1.0821e+00,  4.2555e-03,\n",
       "         -2.8702e-02, -3.5220e+00, -2.3090e+00, -6.7767e-01, -4.1828e-01,\n",
       "         -7.1692e+01, -2.2520e-01,  1.2092e-02, -8.7981e-01, -3.4437e-01,\n",
       "         -1.0390e-01, -6.4949e-01, -3.3275e-02, -2.7531e-01, -1.3260e+00,\n",
       "         -2.1608e-01, -4.9054e-01, -3.6699e-01, -1.3111e+00, -5.2593e-01,\n",
       "         -4.4454e+00, -8.3285e-01, -1.0195e+00,  6.5973e-02, -6.4688e-01,\n",
       "         -3.4707e-01, -2.1931e-01, -4.2201e-01, -3.0579e-01, -1.2811e+00,\n",
       "         -1.6409e-01, -1.0673e+00,  1.6529e-01, -3.9110e-01, -1.0196e+01,\n",
       "         -3.1219e-01, -2.7379e+00, -5.4618e-02, -4.0282e-01, -6.6204e-01,\n",
       "         -8.9593e-01, -5.6248e-01, -2.8347e+00, -1.1034e-01, -7.2864e-01,\n",
       "         -1.0816e+00, -1.6863e+00,  2.2045e+02, -6.3646e-01, -2.9563e+00,\n",
       "         -2.9134e-01, -4.3018e-01,  1.7629e-01, -4.8417e-01, -3.5092e-01,\n",
       "         -4.5925e-01, -3.6215e-01, -2.0373e+00, -1.1797e+00, -5.9761e+00,\n",
       "         -3.3980e+00,  5.6844e-01, -1.3635e+00, -3.0896e-01, -1.9325e+00,\n",
       "         -2.8464e-01, -1.8219e+00, -3.1269e-01, -3.8086e-01, -5.0204e-01,\n",
       "         -1.0639e+00, -1.2257e+00, -1.7788e+00, -5.6647e-01, -3.2496e-01,\n",
       "         -3.0050e-01,  2.4296e-01, -3.7271e-01, -1.8127e-02, -1.5393e-01,\n",
       "         -5.1271e-02, -2.3574e+00, -7.9627e-01, -1.9423e-01, -4.0526e+00,\n",
       "         -3.6532e-01, -7.5691e-01,  5.6319e-02, -1.0541e+00, -4.1115e-01,\n",
       "         -9.4621e-01, -5.9588e-01, -8.5145e-01, -9.9432e-02, -3.0881e-01]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from monai.losses import DiceFocalLoss\n",
    "loss = make_newton_loss(DiceFocalLoss(softmax=True), tik_l=1e-2, use_torch_func=False)\n",
    "\n",
    "input = torch.randn(1,100, requires_grad=True, device='cuda')\n",
    "l = loss(input, torch.randn(1,100, requires_grad=True, device='cuda'))\n",
    "print(l)\n",
    "l.backward()\n",
    "input.grad\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
